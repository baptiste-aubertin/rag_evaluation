from llama_index.llms.groq import Groq
import os

grok_api_key = os.getenv("GROK_API_KEY")
# llm = Groq(model="llama3-70b-8192", api_key=grok_api_key)
model = os.getenv("GROK_LLM_MODEL")

llm = Groq(model=model, api_key=grok_api_key)


def evaluate_instruction_response(
    instruction: str, response: str, reference_answer: str
) -> tuple[str, int]:
    """
    Evaluates the response to an instruction against a reference answer using a language model.
    Args:
        instruction (str): The instruction given to the model.
        response (str): The response generated by the model.
        reference_answer (str): The reference answer to compare against.
    Returns:
        tuple: A tuple containing feedback (str) and score (int). If an error occurs, returns the error message (str) and None for the score.
    """

    try:
        chat_message = EVALUATION_PROMPT.format(
            instruction=instruction,
            response=response,
            reference_answer=reference_answer,
        )

        response = llm.complete(chat_message)

        response = response.text

        feedback, score = response.split("[RESULT]")

        # Normalize the score to be between 0 and 1
        score = (int(score.strip()) - 1) / 4

        return feedback, score
    except Exception as e:
        return str(e), None


EVALUATION_PROMPT = """###Task Description:
An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.
1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.
2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.
3. The output format should look as follows: \"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\"
4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.

###The instruction to evaluate:
{instruction}

###Response to evaluate:
{response}

###Reference Answer (Score 5):
{reference_answer}

###Score Rubrics:
[Is the response correct, accurate, and factual based on the reference answer?]
Score 1: The response is completely incorrect, inaccurate, and/or not factual.
Score 2: The response is mostly incorrect, inaccurate, and/or not factual.
Score 3: The response is somewhat correct, accurate, and/or factual.
Score 4: The response is mostly correct, accurate, and factual.
Score 5: The response is completely correct, accurate, and factual.

###Feedback:"""
